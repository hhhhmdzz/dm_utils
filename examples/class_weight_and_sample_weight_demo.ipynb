{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x, y = load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = {0: 1, 1: 2}\n",
    "sample_weight = ytrain.map(class_weight).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.8881578947368421\n",
      "score: 0.9013157894736842\n",
      "score: 0.9013157894736842\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier(class_weight=None, random_state=42)\n",
    "model.fit(xtrain, ytrain, sample_weight=None)\n",
    "print('score:', model.score(xtest, ytest))\n",
    "\n",
    "# class_weight\n",
    "model = DecisionTreeClassifier(class_weight=class_weight, random_state=42)\n",
    "model.fit(xtrain, ytrain, sample_weight=None)\n",
    "print('score:', model.score(xtest, ytest))\n",
    "ypred1 = model.predict_proba(xtest)\n",
    "\n",
    "# sample_weight\n",
    "model = DecisionTreeClassifier(class_weight=None, random_state=42)\n",
    "model.fit(xtrain, ytrain, sample_weight=sample_weight)\n",
    "print('score:', model.score(xtest, ytest))\n",
    "ypred2 = model.predict_proba(xtest)\n",
    "\n",
    "print((ypred1 == ypred2).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.9473684210526315\n",
      "score: 0.9451754385964912\n",
      "score: 0.9451754385964912\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(class_weight=None, random_state=42)\n",
    "model.fit(xtrain, ytrain, sample_weight=None)\n",
    "print('score:', model.score(xtest, ytest))\n",
    "\n",
    "# class_weight\n",
    "model = RandomForestClassifier(class_weight=class_weight, random_state=42)\n",
    "model.fit(xtrain, ytrain, sample_weight=None)\n",
    "print('score:', model.score(xtest, ytest))\n",
    "ypred1 = model.predict_proba(xtest)\n",
    "\n",
    "# sample_weight\n",
    "model = RandomForestClassifier(class_weight=None, random_state=42)\n",
    "model.fit(xtrain, ytrain, sample_weight=sample_weight)\n",
    "print('score:', model.score(xtest, ytest))\n",
    "ypred2 = model.predict_proba(xtest)\n",
    "\n",
    "print((ypred1 == ypred2).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.9495614035087719\n",
      "score: 0.9627192982456141\n",
      "score: 0.9627192982456141\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "model = XGBClassifier(scale_pos_weight=None, random_state=42)\n",
    "model.fit(xtrain, ytrain, sample_weight=None)\n",
    "print('score:', model.score(xtest, ytest))\n",
    "\n",
    "# class_weight\n",
    "model = XGBClassifier(scale_pos_weight=class_weight[1], random_state=42)\n",
    "model.fit(xtrain, ytrain, sample_weight=None)\n",
    "print('score:', model.score(xtest, ytest))\n",
    "ypred1 = model.predict_proba(xtest)\n",
    "\n",
    "# sample_weight\n",
    "model = XGBClassifier(scale_pos_weight=None, random_state=42)\n",
    "model.fit(xtrain, ytrain, sample_weight=sample_weight)\n",
    "print('score:', model.score(xtest, ytest))\n",
    "ypred2 = model.predict_proba(xtest)\n",
    "\n",
    "print((ypred1 == ypred2).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.9495614035087719\n",
      "score: 0.956140350877193\n",
      "score: 0.956140350877193\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "model = xgb.train(\n",
    "    {\n",
    "        'objective': 'binary:logistic',\n",
    "        'scale_pos_weight': None,\n",
    "    },\n",
    "    dtrain=xgb.DMatrix(xtrain, label=ytrain, weight=None),\n",
    ")\n",
    "ypred = model.predict(xgb.DMatrix(xtest))\n",
    "print('score:', accuracy_score(ytest, ypred > 0.5))\n",
    "\n",
    "model = xgb.train(\n",
    "    {\n",
    "        'objective': 'binary:logistic',\n",
    "        'scale_pos_weight': class_weight[1],\n",
    "    },\n",
    "    dtrain=xgb.DMatrix(xtrain, label=ytrain, weight=None),\n",
    ")\n",
    "ypred1 = model.predict(xgb.DMatrix(xtest))\n",
    "print('score:', accuracy_score(ytest, ypred1 > 0.5))\n",
    "\n",
    "model = xgb.train(\n",
    "    {\n",
    "        'objective': 'binary:logistic',\n",
    "        'scale_pos_weight': None,\n",
    "    },\n",
    "    dtrain=xgb.DMatrix(xtrain, label=ytrain, weight=sample_weight),\n",
    ")\n",
    "ypred2 = model.predict(xgb.DMatrix(xtest))\n",
    "print('score:', accuracy_score(ytest, ypred2 > 0.5))\n",
    "\n",
    "print((ypred1 == ypred2).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.9495614035087719\n",
      "score: 0.9517543859649122\n",
      "score: 0.9517543859649122\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "model = LGBMClassifier(scale_pos_weight=None, random_state=42, verbosity=-1)\n",
    "model.fit(xtrain, ytrain, sample_weight=None)\n",
    "print('score:', model.score(xtest, ytest))\n",
    "\n",
    "# class_weight\n",
    "model = LGBMClassifier(scale_pos_weight=class_weight[1], random_state=42, verbosity=-1)\n",
    "model.fit(xtrain, ytrain, sample_weight=None)\n",
    "print('score:', model.score(xtest, ytest))\n",
    "ypred1 = model.predict_proba(xtest)\n",
    "\n",
    "# sample_weight\n",
    "model = LGBMClassifier(scale_pos_weight=None, random_state=42, verbosity=-1)\n",
    "model.fit(xtrain, ytrain, sample_weight=sample_weight)\n",
    "print('score:', model.score(xtest, ytest))\n",
    "ypred2 = model.predict_proba(xtest)\n",
    "\n",
    "print((ypred1 == ypred2).all())  # !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.9495614035087719\n",
      "score: 0.9517543859649122\n",
      "score: 0.9517543859649122\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "model = LGBMClassifier(class_weight=None, random_state=42, verbosity=-1)\n",
    "model.fit(xtrain, ytrain, sample_weight=None)\n",
    "print('score:', model.score(xtest, ytest))\n",
    "\n",
    "# class_weight\n",
    "model = LGBMClassifier(class_weight=class_weight, random_state=42, verbosity=-1)\n",
    "model.fit(xtrain, ytrain, sample_weight=None)\n",
    "print('score:', model.score(xtest, ytest))\n",
    "ypred1 = model.predict_proba(xtest)\n",
    "\n",
    "# sample_weight\n",
    "model = LGBMClassifier(class_weight=None, random_state=42, verbosity=-1)\n",
    "model.fit(xtrain, ytrain, sample_weight=sample_weight)\n",
    "print('score:', model.score(xtest, ytest))\n",
    "ypred2 = model.predict_proba(xtest)\n",
    "\n",
    "print((ypred1 == ypred2).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.9495614035087719\n",
      "score: 0.9517543859649122\n",
      "score: 0.9517543859649122\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "model = lgb.train(\n",
    "    {\n",
    "        'objective': 'binary',\n",
    "        'scale_pos_weight': None,\n",
    "        'verbosity': -1,\n",
    "    },\n",
    "    train_set=lgb.Dataset(xtrain, label=ytrain, weight=None),\n",
    ")\n",
    "ypred = model.predict(xtest)\n",
    "print('score:', accuracy_score(ytest, ypred > 0.5))\n",
    "\n",
    "model = lgb.train(\n",
    "    {\n",
    "        'objective': 'binary',\n",
    "        'scale_pos_weight': class_weight[1],\n",
    "        'verbosity': -1,\n",
    "    },\n",
    "    train_set=lgb.Dataset(xtrain, label=ytrain, weight=None),\n",
    ")\n",
    "ypred1 = model.predict(xtest)\n",
    "print('score:', accuracy_score(ytest, ypred1 > 0.5))\n",
    "\n",
    "model = lgb.train(\n",
    "    {\n",
    "        'objective': 'binary',\n",
    "        'scale_pos_weight': None,\n",
    "        'verbosity': -1,\n",
    "    },\n",
    "    train_set=lgb.Dataset(xtrain, label=ytrain, weight=sample_weight),\n",
    ")\n",
    "ypred2 = model.predict(xtest)\n",
    "print('score:', accuracy_score(ytest, ypred2 > 0.5))\n",
    "\n",
    "print((ypred1 == ypred2).all())  # !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.9627192982456141\n",
      "score: 0.9605263157894737\n",
      "score: 0.9605263157894737\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "model = CatBoostClassifier(iterations=100, scale_pos_weight=None, random_state=42, logging_level='Silent')\n",
    "model.fit(xtrain, ytrain, sample_weight=None)\n",
    "print('score:', model.score(xtest, ytest))\n",
    "\n",
    "# class_weight\n",
    "model = CatBoostClassifier(iterations=100, scale_pos_weight=class_weight[1], random_state=42, logging_level='Silent')\n",
    "model.fit(xtrain, ytrain, sample_weight=None)\n",
    "print('score:', model.score(xtest, ytest))\n",
    "ypred1 = model.predict_proba(xtest)\n",
    "\n",
    "# sample_weight\n",
    "model = CatBoostClassifier(iterations=100, scale_pos_weight=None, random_state=42, logging_level='Silent')\n",
    "model.fit(xtrain, ytrain, sample_weight=sample_weight)\n",
    "print('score:', model.score(xtest, ytest))\n",
    "ypred2 = model.predict_proba(xtest)\n",
    "\n",
    "print((ypred1 == ypred2).all())  # !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.9627192982456141\n",
      "score: 0.9605263157894737\n",
      "score: 0.9605263157894737\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "model = CatBoostClassifier(iterations=100, class_weights=None, random_state=42, logging_level='Silent')\n",
    "model.fit(xtrain, ytrain, sample_weight=None)\n",
    "print('score:', model.score(xtest, ytest))\n",
    "\n",
    "# class_weight\n",
    "model = CatBoostClassifier(iterations=100, class_weights=class_weight, random_state=42, logging_level='Silent')\n",
    "model.fit(xtrain, ytrain, sample_weight=None)\n",
    "print('score:', model.score(xtest, ytest))\n",
    "ypred1 = model.predict_proba(xtest)\n",
    "\n",
    "# sample_weight\n",
    "model = CatBoostClassifier(iterations=100, class_weights=None, random_state=42, logging_level='Silent')\n",
    "model.fit(xtrain, ytrain, sample_weight=sample_weight)\n",
    "print('score:', model.score(xtest, ytest))\n",
    "ypred2 = model.predict_proba(xtest)\n",
    "\n",
    "print((ypred1 == ypred2).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.9495614035087719\n",
      "score: 0.9583333333333334\n",
      "score: 0.9583333333333334\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "model = cb.train(\n",
    "    params={\n",
    "        'loss_function': 'Logloss',\n",
    "        'scale_pos_weight': 1.0,\n",
    "        'logging_level': 'Silent',\n",
    "    },\n",
    "    dtrain=cb.Pool(xtrain, label=ytrain, weight=None),\n",
    "    iterations=100,\n",
    ")\n",
    "ypred = model.predict(xtest)\n",
    "print('score:', accuracy_score(ytest, ypred > 0.5))\n",
    "\n",
    "model = cb.train(\n",
    "    params={\n",
    "        'loss_function': 'Logloss',\n",
    "        'scale_pos_weight': class_weight[1],\n",
    "        'logging_level': 'Silent',\n",
    "    },\n",
    "    dtrain=cb.Pool(xtrain, label=ytrain, weight=None),\n",
    "    iterations=100,\n",
    ")\n",
    "ypred1 = model.predict(xtest)\n",
    "print('score:', accuracy_score(ytest, ypred1 > 0.5))\n",
    "\n",
    "model = cb.train(\n",
    "    params={\n",
    "        'loss_function': 'Logloss',\n",
    "        'scale_pos_weight': 1.0,\n",
    "        'logging_level': 'Silent',\n",
    "    },\n",
    "    dtrain=cb.Pool(xtrain, label=ytrain, weight=sample_weight),\n",
    "    iterations=100,\n",
    ")\n",
    "ypred2 = model.predict(xtest)\n",
    "print('score:', accuracy_score(ytest, ypred2 > 0.5))\n",
    "\n",
    "print((ypred1 == ypred2).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.9495614035087719\n",
      "score: 0.9583333333333334\n",
      "score: 0.9583333333333334\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "model = cb.train(\n",
    "    params={\n",
    "        'loss_function': 'Logloss',\n",
    "        'class_weights': [1, 1],\n",
    "        'logging_level': 'Silent',\n",
    "    },\n",
    "    dtrain=cb.Pool(xtrain, label=ytrain, weight=None),\n",
    "    iterations=100,\n",
    ")\n",
    "ypred = model.predict(xtest)\n",
    "print('score:', accuracy_score(ytest, ypred > 0.5))\n",
    "\n",
    "model = cb.train(\n",
    "    params={\n",
    "        'loss_function': 'Logloss',\n",
    "        'class_weights': class_weight,\n",
    "        'logging_level': 'Silent',\n",
    "    },\n",
    "    dtrain=cb.Pool(xtrain, label=ytrain, weight=None),\n",
    "    iterations=100,\n",
    ")\n",
    "ypred1 = model.predict(xtest)\n",
    "print('score:', accuracy_score(ytest, ypred1 > 0.5))\n",
    "\n",
    "model = cb.train(\n",
    "    params={\n",
    "        'loss_function': 'Logloss',\n",
    "        'class_weights': [1, 1],\n",
    "        'logging_level': 'Silent',\n",
    "    },\n",
    "    dtrain=cb.Pool(xtrain, label=ytrain, weight=sample_weight),\n",
    "    iterations=100,\n",
    ")\n",
    "ypred2 = model.predict(xtest)\n",
    "print('score:', accuracy_score(ytest, ypred2 > 0.5))\n",
    "\n",
    "print((ypred1 == ypred2).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Hold-Out Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hemingze/miniconda3/envs/mathm/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[CONFLICT] model_str 'dt' is not supported when sklearn_api=False, automatically use sklearn api.\u001b[0m\n",
      "\u001b[33m[CONFLICT] model_str 'rf' is not supported when sklearn_api=False, automatically use sklearn api.\u001b[0m\n",
      "\u001b[33m[CONFLICT] model_str 'ngb' is not supported when sklearn_api=False, automatically use sklearn api.\u001b[0m\n",
      "\u001b[32m[INFO] not provided X_valid and y_valid, auto split train set into train and valid.\u001b[0m\n",
      "\u001b[32m[INFO] hold-out method training begin.\u001b[0m\n",
      "\u001b[32m[INFO] Model DecisionTreeClassifier 1 / 6 training begin.\u001b[0m\n",
      "\u001b[32m[INFO] Model DecisionTreeClassifier 1 / 6 training finish, cost time 0.003 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 1 / 6 model validation scores: {'acc': 0.9565217391304348, 'model': 'DecisionTreeClassifier'}\u001b[0m\n",
      "\u001b[32m[INFO] Model RandomForestClassifier 2 / 6 training begin.\u001b[0m\n",
      "\u001b[32m[INFO] Model RandomForestClassifier 2 / 6 training finish, cost time 0.069 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 2 / 6 model validation scores: {'acc': 0.9130434782608695, 'model': 'RandomForestClassifier'}\u001b[0m\n",
      "\u001b[32m[INFO] Model XGBoost 3 / 6 training begin.\u001b[0m\n",
      "[0]\tvalid-auc:0.90873\n",
      "[99]\tvalid-auc:0.96032\n",
      "\u001b[32m[INFO] Model XGBoost 3 / 6 training finish, cost time 0.988 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 3 / 6 model validation scores: {'acc': 0.9565217391304348, 'model': 'XGBoost'}\u001b[0m\n",
      "\u001b[32m[INFO] Model LightGBM 4 / 6 training begin.\u001b[0m\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hemingze/miniconda3/envs/mathm/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's auc: 0.940476\tvalid_0's binary_logloss: 0.396679\tvalid_0's binary_error: 0.0434783\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\tvalid_0's auc: 0.944444\tvalid_0's binary_logloss: 0.665593\tvalid_0's binary_error: 0.391304\n",
      "\u001b[32m[INFO] Model LightGBM 4 / 6 training finish, cost time 0.785 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 4 / 6 model validation scores: {'acc': 0.6086956521739131, 'model': 'LightGBM'}\u001b[0m\n",
      "\u001b[32m[INFO] Model CatBoost 5 / 6 training begin.\u001b[0m\n",
      "0:\ttest: 0.9722222\tbest: 0.9722222 (0)\ttotal: 1.1ms\tremaining: 109ms\n",
      "99:\ttest: 0.9444444\tbest: 0.9722222 (0)\ttotal: 91.8ms\tremaining: 0us\n",
      "\n",
      "bestTest = 0.9722222222\n",
      "bestIteration = 0\n",
      "\n",
      "Shrink model to first 1 iterations.\n",
      "\u001b[32m[INFO] Model CatBoost 5 / 6 training finish, cost time 0.143 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 5 / 6 model validation scores: {'acc': 0.8695652173913043, 'model': 'CatBoost'}\u001b[0m\n",
      "\u001b[32m[INFO] Model NGBClassifier 6 / 6 training begin.\u001b[0m\n",
      "[iter 0] loss=0.6802 val_loss=0.6457 scale=512.0000 norm=249.4059\n",
      "\u001b[32m[INFO] Model NGBClassifier 6 / 6 training finish, cost time 0.148 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 6 / 6 model validation scores: {'acc': 0.9565217391304348, 'model': 'NGBClassifier'}\u001b[0m\n",
      "\u001b[32m[INFO] hold-out method training finish, cost time 2.14 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] total 6 model validation scores:{'acc': 0.9565217391304348, 'model': 'CatBoost,DecisionTreeClassifier,LightGBM,NGBClassifier,RandomForestClassifier,XGBoost'}\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model0</th>\n",
       "      <td>0.956522</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model1</th>\n",
       "      <td>0.913043</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model2</th>\n",
       "      <td>0.956522</td>\n",
       "      <td>XGBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model3</th>\n",
       "      <td>0.608696</td>\n",
       "      <td>LightGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model4</th>\n",
       "      <td>0.869565</td>\n",
       "      <td>CatBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model5</th>\n",
       "      <td>0.956522</td>\n",
       "      <td>NGBClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>0.956522</td>\n",
       "      <td>CatBoost,DecisionTreeClassifier,LightGBM,NGBCl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             acc                                              model\n",
       "model0  0.956522                             DecisionTreeClassifier\n",
       "model1  0.913043                             RandomForestClassifier\n",
       "model2  0.956522                                            XGBoost\n",
       "model3  0.608696                                           LightGBM\n",
       "model4  0.869565                                           CatBoost\n",
       "model5  0.956522                                      NGBClassifier\n",
       "all     0.956522  CatBoost,DecisionTreeClassifier,LightGBM,NGBCl..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dm_utils.hom import HOM\n",
    "\n",
    "model_params = [None]*6\n",
    "model_params[-1] = {'natural_gradient': False}\n",
    "hom = HOM(task='cls', model=['dt', 'rf', 'xgb', 'lgb', 'cb', 'ngb'], epochs=100)\n",
    "hom.fit(xtrain, ytrain, record_time=True, model_params=model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[CONFLICT] model_str 'dt' is not supported when sklearn_api=False, automatically use sklearn api.\u001b[0m\n",
      "\u001b[33m[CONFLICT] model_str 'rf' is not supported when sklearn_api=False, automatically use sklearn api.\u001b[0m\n",
      "\u001b[33m[CONFLICT] model_str 'ngb' is not supported when sklearn_api=False, automatically use sklearn api.\u001b[0m\n",
      "\u001b[32m[INFO] not provided X_valid and y_valid, auto split train set into train and valid.\u001b[0m\n",
      "\u001b[32m[INFO] hold-out method training begin.\u001b[0m\n",
      "\u001b[32m[INFO] Model DecisionTreeClassifier 1 / 6 training begin.\u001b[0m\n",
      "\u001b[32m[INFO] Model DecisionTreeClassifier 1 / 6 training finish, cost time 0.003 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 1 / 6 model validation scores: {'acc': 0.9130434782608695, 'model': 'DecisionTreeClassifier'}\u001b[0m\n",
      "\u001b[32m[INFO] Model RandomForestClassifier 2 / 6 training begin.\u001b[0m\n",
      "\u001b[32m[INFO] Model RandomForestClassifier 2 / 6 training finish, cost time 0.083 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 2 / 6 model validation scores: {'acc': 0.9565217391304348, 'model': 'RandomForestClassifier'}\u001b[0m\n",
      "\u001b[32m[INFO] Model XGBoost 3 / 6 training begin.\u001b[0m\n",
      "[0]\tvalid-auc:0.90873\n",
      "[99]\tvalid-auc:0.96032\n",
      "\u001b[32m[INFO] Model XGBoost 3 / 6 training finish, cost time 3.041 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 3 / 6 model validation scores: {'acc': 0.6086956521739131, 'model': 'XGBoost'}\u001b[0m\n",
      "\u001b[32m[INFO] Model LightGBM 4 / 6 training begin.\u001b[0m\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hemingze/miniconda3/envs/mathm/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's auc: 0.968254\tvalid_0's binary_logloss: 0.311995\tvalid_0's binary_error: 0.027027\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[94]\tvalid_0's auc: 0.972222\tvalid_0's binary_logloss: 0.31978\tvalid_0's binary_error: 0.0540541\n",
      "\u001b[32m[INFO] Model LightGBM 4 / 6 training finish, cost time 2.933 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 4 / 6 model validation scores: {'acc': 0.9130434782608695, 'model': 'LightGBM'}\u001b[0m\n",
      "\u001b[32m[INFO] Model CatBoost 5 / 6 training begin.\u001b[0m\n",
      "0:\ttest: 0.9563492\tbest: 0.9563492 (0)\ttotal: 1.04ms\tremaining: 103ms\n",
      "99:\ttest: 0.9365079\tbest: 0.9920635 (1)\ttotal: 85.8ms\tremaining: 0us\n",
      "\n",
      "bestTest = 0.9920634921\n",
      "bestIteration = 1\n",
      "\n",
      "Shrink model to first 2 iterations.\n",
      "\u001b[32m[INFO] Model CatBoost 5 / 6 training finish, cost time 0.105 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 5 / 6 model validation scores: {'acc': 0.9130434782608695, 'model': 'CatBoost'}\u001b[0m\n",
      "\u001b[32m[INFO] Model NGBClassifier 6 / 6 training begin.\u001b[0m\n",
      "[iter 0] loss=0.6515 val_loss=0.3858 scale=512.0000 norm=252.8395\n",
      "\u001b[32m[INFO] Model NGBClassifier 6 / 6 training finish, cost time 0.158 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 6 / 6 model validation scores: {'acc': 0.9130434782608695, 'model': 'NGBClassifier'}\u001b[0m\n",
      "\u001b[32m[INFO] hold-out method training finish, cost time 6.323 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] total 6 model validation scores:{'acc': 0.9130434782608695, 'model': 'CatBoost,DecisionTreeClassifier,LightGBM,NGBClassifier,RandomForestClassifier,XGBoost'}\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model0</th>\n",
       "      <td>0.913043</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model1</th>\n",
       "      <td>0.956522</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model2</th>\n",
       "      <td>0.608696</td>\n",
       "      <td>XGBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model3</th>\n",
       "      <td>0.913043</td>\n",
       "      <td>LightGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model4</th>\n",
       "      <td>0.913043</td>\n",
       "      <td>CatBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model5</th>\n",
       "      <td>0.913043</td>\n",
       "      <td>NGBClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>0.913043</td>\n",
       "      <td>CatBoost,DecisionTreeClassifier,LightGBM,NGBCl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             acc                                              model\n",
       "model0  0.913043                             DecisionTreeClassifier\n",
       "model1  0.956522                             RandomForestClassifier\n",
       "model2  0.608696                                            XGBoost\n",
       "model3  0.913043                                           LightGBM\n",
       "model4  0.913043                                           CatBoost\n",
       "model5  0.913043                                      NGBClassifier\n",
       "all     0.913043  CatBoost,DecisionTreeClassifier,LightGBM,NGBCl..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hom = HOM(task='cls', model=['dt', 'rf', 'xgb', 'lgb', 'cb', 'ngb'], epochs=100)\n",
    "hom.fit(xtrain, ytrain, record_time=True, class_weight=class_weight, model_params=model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[CONFLICT] model_str 'dt' is not supported when sklearn_api=False, automatically use sklearn api.\u001b[0m\n",
      "\u001b[33m[CONFLICT] model_str 'rf' is not supported when sklearn_api=False, automatically use sklearn api.\u001b[0m\n",
      "\u001b[33m[CONFLICT] model_str 'ngb' is not supported when sklearn_api=False, automatically use sklearn api.\u001b[0m\n",
      "\u001b[32m[INFO] not provided X_valid and y_valid, auto split train set into train and valid.\u001b[0m\n",
      "\u001b[32m[INFO] hold-out method training begin.\u001b[0m\n",
      "\u001b[32m[INFO] Model DecisionTreeClassifier 1 / 6 training begin.\u001b[0m\n",
      "\u001b[32m[INFO] Model DecisionTreeClassifier 1 / 6 training finish, cost time 0.002 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 1 / 6 model validation scores: {'acc': 0.9130434782608695, 'model': 'DecisionTreeClassifier'}\u001b[0m\n",
      "\u001b[32m[INFO] Model RandomForestClassifier 2 / 6 training begin.\u001b[0m\n",
      "\u001b[32m[INFO] Model RandomForestClassifier 2 / 6 training finish, cost time 0.068 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 2 / 6 model validation scores: {'acc': 0.9565217391304348, 'model': 'RandomForestClassifier'}\u001b[0m\n",
      "\u001b[32m[INFO] Model XGBoost 3 / 6 training begin.\u001b[0m\n",
      "[0]\tvalid-auc:0.90873\n",
      "[99]\tvalid-auc:0.96032\n",
      "\u001b[32m[INFO] Model XGBoost 3 / 6 training finish, cost time 1.365 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 3 / 6 model validation scores: {'acc': 0.6086956521739131, 'model': 'XGBoost'}\u001b[0m\n",
      "\u001b[32m[INFO] Model LightGBM 4 / 6 training begin.\u001b[0m\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hemingze/miniconda3/envs/mathm/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's auc: 0.968254\tvalid_0's binary_logloss: 0.311995\tvalid_0's binary_error: 0.027027\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[94]\tvalid_0's auc: 0.972222\tvalid_0's binary_logloss: 0.31978\tvalid_0's binary_error: 0.0540541\n",
      "\u001b[32m[INFO] Model LightGBM 4 / 6 training finish, cost time 1.303 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 4 / 6 model validation scores: {'acc': 0.9130434782608695, 'model': 'LightGBM'}\u001b[0m\n",
      "\u001b[32m[INFO] Model CatBoost 5 / 6 training begin.\u001b[0m\n",
      "0:\ttest: 0.9563492\tbest: 0.9563492 (0)\ttotal: 991us\tremaining: 98.1ms\n",
      "99:\ttest: 0.9365079\tbest: 0.9920635 (1)\ttotal: 89ms\tremaining: 0us\n",
      "\n",
      "bestTest = 0.9920634921\n",
      "bestIteration = 1\n",
      "\n",
      "Shrink model to first 2 iterations.\n",
      "\u001b[32m[INFO] Model CatBoost 5 / 6 training finish, cost time 0.145 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 5 / 6 model validation scores: {'acc': 0.9130434782608695, 'model': 'CatBoost'}\u001b[0m\n",
      "\u001b[32m[INFO] Model NGBClassifier 6 / 6 training begin.\u001b[0m\n",
      "[iter 0] loss=0.5832 val_loss=0.6593 scale=16.0000 norm=32.0000\n",
      "\u001b[32m[INFO] Model NGBClassifier 6 / 6 training finish, cost time 0.14 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 6 / 6 model validation scores: {'acc': 0.9130434782608695, 'model': 'NGBClassifier'}\u001b[0m\n",
      "\u001b[32m[INFO] hold-out method training finish, cost time 3.024 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] total 6 model validation scores:{'acc': 0.9130434782608695, 'model': 'CatBoost,DecisionTreeClassifier,LightGBM,NGBClassifier,RandomForestClassifier,XGBoost'}\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model0</th>\n",
       "      <td>0.913043</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model1</th>\n",
       "      <td>0.956522</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model2</th>\n",
       "      <td>0.608696</td>\n",
       "      <td>XGBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model3</th>\n",
       "      <td>0.913043</td>\n",
       "      <td>LightGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model4</th>\n",
       "      <td>0.913043</td>\n",
       "      <td>CatBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model5</th>\n",
       "      <td>0.913043</td>\n",
       "      <td>NGBClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>0.913043</td>\n",
       "      <td>CatBoost,DecisionTreeClassifier,LightGBM,NGBCl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             acc                                              model\n",
       "model0  0.913043                             DecisionTreeClassifier\n",
       "model1  0.956522                             RandomForestClassifier\n",
       "model2  0.608696                                            XGBoost\n",
       "model3  0.913043                                           LightGBM\n",
       "model4  0.913043                                           CatBoost\n",
       "model5  0.913043                                      NGBClassifier\n",
       "all     0.913043  CatBoost,DecisionTreeClassifier,LightGBM,NGBCl..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hom = HOM(task='cls', model=['dt', 'rf', 'xgb', 'lgb', 'cb', 'ngb'], epochs=100)\n",
    "hom.fit(xtrain, ytrain, record_time=True, weight_train=sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[CONFLICT] model_str 'dt' is not supported when sklearn_api=False, automatically use sklearn api.\u001b[0m\n",
      "\u001b[33m[CONFLICT] model_str 'rf' is not supported when sklearn_api=False, automatically use sklearn api.\u001b[0m\n",
      "\u001b[33m[CONFLICT] model_str 'ngb' is not supported when sklearn_api=False, automatically use sklearn api.\u001b[0m\n",
      "\u001b[32m[INFO] not provided X_valid and y_valid, auto split train set into train and valid.\u001b[0m\n",
      "\u001b[32m[INFO] hold-out method training begin.\u001b[0m\n",
      "\u001b[32m[INFO] Model DecisionTreeClassifier 1 / 6 training begin.\u001b[0m\n",
      "\u001b[32m[INFO] Model DecisionTreeClassifier 1 / 6 training finish, cost time 0.002 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 1 / 6 model validation scores: {'acc': 0.9130434782608695, 'model': 'DecisionTreeClassifier'}\u001b[0m\n",
      "\u001b[32m[INFO] Model RandomForestClassifier 2 / 6 training begin.\u001b[0m\n",
      "\u001b[32m[INFO] Model RandomForestClassifier 2 / 6 training finish, cost time 0.069 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 2 / 6 model validation scores: {'acc': 0.9565217391304348, 'model': 'RandomForestClassifier'}\u001b[0m\n",
      "\u001b[32m[INFO] Model XGBoost 3 / 6 training begin.\u001b[0m\n",
      "[0]\tvalid-auc:0.94444\n",
      "[99]\tvalid-auc:0.96032\n",
      "\u001b[32m[INFO] Model XGBoost 3 / 6 training finish, cost time 0.456 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 3 / 6 model validation scores: {'acc': 0.6086956521739131, 'model': 'XGBoost'}\u001b[0m\n",
      "\u001b[32m[INFO] Model LightGBM 4 / 6 training begin.\u001b[0m\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hemingze/miniconda3/envs/mathm/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's auc: 0.944444\tvalid_0's binary_logloss: 0.215296\tvalid_0's binary_error: 0.138462\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[69]\tvalid_0's auc: 0.968254\tvalid_0's binary_logloss: 0.249411\tvalid_0's binary_error: 0.138462\n",
      "\u001b[32m[INFO] Model LightGBM 4 / 6 training finish, cost time 1.335 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 4 / 6 model validation scores: {'acc': 0.6086956521739131, 'model': 'LightGBM'}\u001b[0m\n",
      "\u001b[32m[INFO] Model CatBoost 5 / 6 training begin.\u001b[0m\n",
      "0:\ttest: 0.9285714\tbest: 0.9285714 (0)\ttotal: 1.68ms\tremaining: 166ms\n",
      "99:\ttest: 0.9285714\tbest: 0.9365079 (40)\ttotal: 86.4ms\tremaining: 0us\n",
      "\n",
      "bestTest = 0.9365079365\n",
      "bestIteration = 40\n",
      "\n",
      "Shrink model to first 41 iterations.\n",
      "\u001b[32m[INFO] Model CatBoost 5 / 6 training finish, cost time 0.107 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 5 / 6 model validation scores: {'acc': 0.9565217391304348, 'model': 'CatBoost'}\u001b[0m\n",
      "\u001b[32m[INFO] Model NGBClassifier 6 / 6 training begin.\u001b[0m\n",
      "[iter 0] loss=0.5782 val_loss=0.3718 scale=128.0000 norm=61.8052\n",
      "\u001b[32m[INFO] Model NGBClassifier 6 / 6 training finish, cost time 0.166 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 6 / 6 model validation scores: {'acc': 0.8695652173913043, 'model': 'NGBClassifier'}\u001b[0m\n",
      "\u001b[32m[INFO] hold-out method training finish, cost time 2.136 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] total 6 model validation scores:{'acc': 0.9565217391304348, 'model': 'CatBoost,DecisionTreeClassifier,LightGBM,NGBClassifier,RandomForestClassifier,XGBoost'}\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model0</th>\n",
       "      <td>0.913043</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model1</th>\n",
       "      <td>0.956522</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model2</th>\n",
       "      <td>0.608696</td>\n",
       "      <td>XGBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model3</th>\n",
       "      <td>0.608696</td>\n",
       "      <td>LightGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model4</th>\n",
       "      <td>0.956522</td>\n",
       "      <td>CatBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model5</th>\n",
       "      <td>0.869565</td>\n",
       "      <td>NGBClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>0.956522</td>\n",
       "      <td>CatBoost,DecisionTreeClassifier,LightGBM,NGBCl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             acc                                              model\n",
       "model0  0.913043                             DecisionTreeClassifier\n",
       "model1  0.956522                             RandomForestClassifier\n",
       "model2  0.608696                                            XGBoost\n",
       "model3  0.608696                                           LightGBM\n",
       "model4  0.956522                                           CatBoost\n",
       "model5  0.869565                                      NGBClassifier\n",
       "all     0.956522  CatBoost,DecisionTreeClassifier,LightGBM,NGBCl..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hom = HOM(task='cls', model=['dt', 'rf', 'xgb', 'lgb', 'cb', 'ngb'], epochs=100)\n",
    "hom.fit(xtrain, ytrain, record_time=True, class_weight=class_weight, weight_train=sample_weight, model_params=model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7. Out-of-Fold Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[CONFLICT] model_str 'dt' is not supported when sklearn_api=False, automatically use sklearn api.\u001b[0m\n",
      "\u001b[33m[CONFLICT] model_str 'ngb' is not supported when sklearn_api=False, automatically use sklearn api.\u001b[0m\n",
      "\u001b[32m[INFO] 5-fold training begin.\u001b[0m\n",
      "\u001b[32m[INFO] Model DecisionTreeClassifier, Fold 1 / 5 training begin.\u001b[0m\n",
      "\u001b[32m[INFO] Model DecisionTreeClassifier, Fold 1 / 5 training finish, cost time 0.003 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 1 / 5 fold validation scores: {'acc': 0.9565217391304348, 'model': 'DecisionTreeClassifier'}\u001b[0m\n",
      "\u001b[32m[INFO] Model XGBoost, Fold 2 / 5 training begin.\u001b[0m\n",
      "[0]\tvalid-auc:0.78462\n",
      "[99]\tvalid-auc:0.95769\n",
      "\u001b[32m[INFO] Model XGBoost, Fold 2 / 5 training finish, cost time 1.297 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 2 / 5 fold validation scores: {'acc': 0.9130434782608695, 'model': 'XGBoost'}\u001b[0m\n",
      "\u001b[32m[INFO] Model LightGBM, Fold 3 / 5 training begin.\u001b[0m\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hemingze/miniconda3/envs/mathm/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's auc: 0.992063\tvalid_0's binary_logloss: 0.405306\tvalid_0's binary_error: 0.0434783\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[18]\tvalid_0's auc: 0.996032\tvalid_0's binary_logloss: 0.597975\tvalid_0's binary_error: 0.391304\n",
      "\u001b[32m[INFO] Model LightGBM, Fold 3 / 5 training finish, cost time 0.387 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 3 / 5 fold validation scores: {'acc': 0.6086956521739131, 'model': 'LightGBM'}\u001b[0m\n",
      "\u001b[32m[INFO] Model CatBoost, Fold 4 / 5 training begin.\u001b[0m\n",
      "0:\ttest: 0.8616071\tbest: 0.8616071 (0)\ttotal: 1.03ms\tremaining: 102ms\n",
      "99:\ttest: 0.9821429\tbest: 1.0000000 (8)\ttotal: 86.2ms\tremaining: 0us\n",
      "\n",
      "bestTest = 1\n",
      "bestIteration = 8\n",
      "\n",
      "Shrink model to first 9 iterations.\n",
      "\u001b[32m[INFO] Model CatBoost, Fold 4 / 5 training finish, cost time 0.142 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 4 / 5 fold validation scores: {'acc': 0.8636363636363636, 'model': 'CatBoost'}\u001b[0m\n",
      "\u001b[32m[INFO] Model NGBClassifier, Fold 5 / 5 training begin.\u001b[0m\n",
      "[iter 0] loss=0.6710 val_loss=0.5939 scale=16.0000 norm=32.0000\n",
      "\u001b[32m[INFO] Model NGBClassifier, Fold 5 / 5 training finish, cost time 0.134 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 5 / 5 fold validation scores: {'acc': 1.0, 'model': 'NGBClassifier'}\u001b[0m\n",
      "\u001b[32m[INFO] 5-fold training finish, cost time 1.964 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] total 5-fold validation scores:{'acc': 0.8672566371681416, 'model': 'CatBoost,DecisionTreeClassifier,LightGBM,NGBClassifier,XGBoost'}\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fold0</th>\n",
       "      <td>0.956522</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold1</th>\n",
       "      <td>0.913043</td>\n",
       "      <td>XGBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold2</th>\n",
       "      <td>0.608696</td>\n",
       "      <td>LightGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold3</th>\n",
       "      <td>0.863636</td>\n",
       "      <td>CatBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NGBClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>0.867257</td>\n",
       "      <td>CatBoost,DecisionTreeClassifier,LightGBM,NGBCl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            acc                                              model\n",
       "fold0  0.956522                             DecisionTreeClassifier\n",
       "fold1  0.913043                                            XGBoost\n",
       "fold2  0.608696                                           LightGBM\n",
       "fold3  0.863636                                           CatBoost\n",
       "fold4       1.0                                      NGBClassifier\n",
       "all    0.867257  CatBoost,DecisionTreeClassifier,LightGBM,NGBCl..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dm_utils import OOF\n",
    "\n",
    "oof = OOF(task='cls', model=['dt', 'xgb', 'lgb', 'cb', 'ngb'], epochs=100)\n",
    "oof.fit(xtrain, ytrain, record_time=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[CONFLICT] model_str 'dt' is not supported when sklearn_api=False, automatically use sklearn api.\u001b[0m\n",
      "\u001b[33m[CONFLICT] model_str 'ngb' is not supported when sklearn_api=False, automatically use sklearn api.\u001b[0m\n",
      "\u001b[32m[INFO] 5-fold training begin.\u001b[0m\n",
      "\u001b[32m[INFO] Model DecisionTreeClassifier, Fold 1 / 5 training begin.\u001b[0m\n",
      "\u001b[32m[INFO] Model DecisionTreeClassifier, Fold 1 / 5 training finish, cost time 0.003 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 1 / 5 fold validation scores: {'acc': 0.9130434782608695, 'model': 'DecisionTreeClassifier'}\u001b[0m\n",
      "\u001b[32m[INFO] Model XGBoost, Fold 2 / 5 training begin.\u001b[0m\n",
      "[0]\tvalid-auc:0.78462\n",
      "[99]\tvalid-auc:0.95769\n",
      "\u001b[32m[INFO] Model XGBoost, Fold 2 / 5 training finish, cost time 2.679 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 2 / 5 fold validation scores: {'acc': 0.5652173913043478, 'model': 'XGBoost'}\u001b[0m\n",
      "\u001b[32m[INFO] Model LightGBM, Fold 3 / 5 training begin.\u001b[0m\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's auc: 0.996032\tvalid_0's binary_logloss: 0.323744\tvalid_0's binary_error: 0.0540541\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[18]\tvalid_0's auc: 0.996032\tvalid_0's binary_logloss: 0.48881\tvalid_0's binary_error: 0.243243\n",
      "\u001b[32m[INFO] Model LightGBM, Fold 3 / 5 training finish, cost time 0.02 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 3 / 5 fold validation scores: {'acc': 0.6086956521739131, 'model': 'LightGBM'}\u001b[0m\n",
      "\u001b[32m[INFO] Model CatBoost, Fold 4 / 5 training begin.\u001b[0m\n",
      "0:\ttest: 0.9196429\tbest: 0.9196429 (0)\ttotal: 1.02ms\tremaining: 101ms\n",
      "99:\ttest: 0.9821429\tbest: 1.0000000 (6)\ttotal: 85ms\tremaining: 0us\n",
      "\n",
      "bestTest = 1\n",
      "bestIteration = 6\n",
      "\n",
      "Shrink model to first 7 iterations.\n",
      "\u001b[32m[INFO] Model CatBoost, Fold 4 / 5 training finish, cost time 0.105 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 4 / 5 fold validation scores: {'acc': 0.9545454545454546, 'model': 'CatBoost'}\u001b[0m\n",
      "\u001b[32m[INFO] Model NGBClassifier, Fold 5 / 5 training begin.\u001b[0m\n",
      "[iter 0] loss=0.5832 val_loss=0.5851 scale=16.0000 norm=32.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hemingze/miniconda3/envs/mathm/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] Model NGBClassifier, Fold 5 / 5 training finish, cost time 0.143 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 5 / 5 fold validation scores: {'acc': 0.9545454545454546, 'model': 'NGBClassifier'}\u001b[0m\n",
      "\u001b[32m[INFO] 5-fold training finish, cost time 2.951 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] total 5-fold validation scores:{'acc': 0.7964601769911505, 'model': 'CatBoost,DecisionTreeClassifier,LightGBM,NGBClassifier,XGBoost'}\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fold0</th>\n",
       "      <td>0.913043</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold1</th>\n",
       "      <td>0.565217</td>\n",
       "      <td>XGBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold2</th>\n",
       "      <td>0.608696</td>\n",
       "      <td>LightGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold3</th>\n",
       "      <td>0.954545</td>\n",
       "      <td>CatBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold4</th>\n",
       "      <td>0.954545</td>\n",
       "      <td>NGBClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>0.79646</td>\n",
       "      <td>CatBoost,DecisionTreeClassifier,LightGBM,NGBCl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            acc                                              model\n",
       "fold0  0.913043                             DecisionTreeClassifier\n",
       "fold1  0.565217                                            XGBoost\n",
       "fold2  0.608696                                           LightGBM\n",
       "fold3  0.954545                                           CatBoost\n",
       "fold4  0.954545                                      NGBClassifier\n",
       "all     0.79646  CatBoost,DecisionTreeClassifier,LightGBM,NGBCl..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof = OOF(task='cls', model=['dt', 'xgb', 'lgb', 'cb', 'ngb'], epochs=100)\n",
    "oof.fit(xtrain, ytrain, record_time=True, class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[CONFLICT] model_str 'dt' is not supported when sklearn_api=False, automatically use sklearn api.\u001b[0m\n",
      "\u001b[33m[CONFLICT] model_str 'ngb' is not supported when sklearn_api=False, automatically use sklearn api.\u001b[0m\n",
      "\u001b[32m[INFO] 5-fold training begin.\u001b[0m\n",
      "\u001b[32m[INFO] Model DecisionTreeClassifier, Fold 1 / 5 training begin.\u001b[0m\n",
      "\u001b[32m[INFO] Model DecisionTreeClassifier, Fold 1 / 5 training finish, cost time 0.003 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 1 / 5 fold validation scores: {'acc': 0.9130434782608695, 'model': 'DecisionTreeClassifier'}\u001b[0m\n",
      "\u001b[32m[INFO] Model XGBoost, Fold 2 / 5 training begin.\u001b[0m\n",
      "[0]\tvalid-auc:0.78462\n",
      "[99]\tvalid-auc:0.95769\n",
      "\u001b[32m[INFO] Model XGBoost, Fold 2 / 5 training finish, cost time 0.202 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 2 / 5 fold validation scores: {'acc': 0.5652173913043478, 'model': 'XGBoost'}\u001b[0m\n",
      "\u001b[32m[INFO] Model LightGBM, Fold 3 / 5 training begin.\u001b[0m\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hemingze/miniconda3/envs/mathm/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's auc: 0.996032\tvalid_0's binary_logloss: 0.323744\tvalid_0's binary_error: 0.0540541\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[18]\tvalid_0's auc: 0.996032\tvalid_0's binary_logloss: 0.48881\tvalid_0's binary_error: 0.243243\n",
      "\u001b[32m[INFO] Model LightGBM, Fold 3 / 5 training finish, cost time 2.658 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 3 / 5 fold validation scores: {'acc': 0.6086956521739131, 'model': 'LightGBM'}\u001b[0m\n",
      "\u001b[32m[INFO] Model CatBoost, Fold 4 / 5 training begin.\u001b[0m\n",
      "0:\ttest: 0.9196429\tbest: 0.9196429 (0)\ttotal: 1.29ms\tremaining: 128ms\n",
      "99:\ttest: 0.9821429\tbest: 1.0000000 (6)\ttotal: 86.2ms\tremaining: 0us\n",
      "\n",
      "bestTest = 1\n",
      "bestIteration = 6\n",
      "\n",
      "Shrink model to first 7 iterations.\n",
      "\u001b[32m[INFO] Model CatBoost, Fold 4 / 5 training finish, cost time 0.136 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 4 / 5 fold validation scores: {'acc': 0.9545454545454546, 'model': 'CatBoost'}\u001b[0m\n",
      "\u001b[32m[INFO] Model NGBClassifier, Fold 5 / 5 training begin.\u001b[0m\n",
      "[iter 0] loss=0.6075 val_loss=0.4957 scale=32.0000 norm=64.0000\n",
      "\u001b[32m[INFO] Model NGBClassifier, Fold 5 / 5 training finish, cost time 0.143 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 5 / 5 fold validation scores: {'acc': 1.0, 'model': 'NGBClassifier'}\u001b[0m\n",
      "\u001b[32m[INFO] 5-fold training finish, cost time 3.141 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] total 5-fold validation scores:{'acc': 0.8053097345132744, 'model': 'CatBoost,DecisionTreeClassifier,LightGBM,NGBClassifier,XGBoost'}\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fold0</th>\n",
       "      <td>0.913043</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold1</th>\n",
       "      <td>0.565217</td>\n",
       "      <td>XGBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold2</th>\n",
       "      <td>0.608696</td>\n",
       "      <td>LightGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold3</th>\n",
       "      <td>0.954545</td>\n",
       "      <td>CatBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NGBClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>0.80531</td>\n",
       "      <td>CatBoost,DecisionTreeClassifier,LightGBM,NGBCl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            acc                                              model\n",
       "fold0  0.913043                             DecisionTreeClassifier\n",
       "fold1  0.565217                                            XGBoost\n",
       "fold2  0.608696                                           LightGBM\n",
       "fold3  0.954545                                           CatBoost\n",
       "fold4       1.0                                      NGBClassifier\n",
       "all     0.80531  CatBoost,DecisionTreeClassifier,LightGBM,NGBCl..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof = OOF(task='cls', model=['dt', 'xgb', 'lgb', 'cb', 'ngb'], epochs=100)\n",
    "oof.fit(xtrain, ytrain, record_time=True, weight_train=sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[CONFLICT] model_str 'dt' is not supported when sklearn_api=False, automatically use sklearn api.\u001b[0m\n",
      "\u001b[33m[CONFLICT] model_str 'ngb' is not supported when sklearn_api=False, automatically use sklearn api.\u001b[0m\n",
      "\u001b[32m[INFO] 5-fold training begin.\u001b[0m\n",
      "\u001b[32m[INFO] Model DecisionTreeClassifier, Fold 1 / 5 training begin.\u001b[0m\n",
      "\u001b[32m[INFO] Model DecisionTreeClassifier, Fold 1 / 5 training finish, cost time 0.003 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 1 / 5 fold validation scores: {'acc': 0.9130434782608695, 'model': 'DecisionTreeClassifier'}\u001b[0m\n",
      "\u001b[32m[INFO] Model XGBoost, Fold 2 / 5 training begin.\u001b[0m\n",
      "[0]\tvalid-auc:0.91154\n",
      "[99]\tvalid-auc:0.96154\n",
      "\u001b[32m[INFO] Model XGBoost, Fold 2 / 5 training finish, cost time 0.389 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 2 / 5 fold validation scores: {'acc': 0.5652173913043478, 'model': 'XGBoost'}\u001b[0m\n",
      "\u001b[32m[INFO] Model LightGBM, Fold 3 / 5 training begin.\u001b[0m\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hemingze/miniconda3/envs/mathm/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's auc: 0.992063\tvalid_0's binary_logloss: 0.225064\tvalid_0's binary_error: 0.138462\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3]\tvalid_0's auc: 0.992063\tvalid_0's binary_logloss: 0.392953\tvalid_0's binary_error: 0.138462\n",
      "\u001b[32m[INFO] Model LightGBM, Fold 3 / 5 training finish, cost time 1.536 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 3 / 5 fold validation scores: {'acc': 0.6086956521739131, 'model': 'LightGBM'}\u001b[0m\n",
      "\u001b[32m[INFO] Model CatBoost, Fold 4 / 5 training begin.\u001b[0m\n",
      "0:\ttest: 0.9642857\tbest: 0.9642857 (0)\ttotal: 978us\tremaining: 96.9ms\n",
      "99:\ttest: 0.9821429\tbest: 0.9910714 (2)\ttotal: 81.3ms\tremaining: 0us\n",
      "\n",
      "bestTest = 0.9910714286\n",
      "bestIteration = 2\n",
      "\n",
      "Shrink model to first 3 iterations.\n",
      "\u001b[32m[INFO] Model CatBoost, Fold 4 / 5 training finish, cost time 0.101 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 4 / 5 fold validation scores: {'acc': 0.9090909090909091, 'model': 'CatBoost'}\u001b[0m\n",
      "\u001b[32m[INFO] Model NGBClassifier, Fold 5 / 5 training begin.\u001b[0m\n",
      "[iter 0] loss=0.5466 val_loss=0.5032 scale=16.0000 norm=32.0000\n",
      "\u001b[32m[INFO] Model NGBClassifier, Fold 5 / 5 training finish, cost time 0.149 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 5 / 5 fold validation scores: {'acc': 0.9545454545454546, 'model': 'NGBClassifier'}\u001b[0m\n",
      "\u001b[32m[INFO] 5-fold training finish, cost time 2.179 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] total 5-fold validation scores:{'acc': 0.7876106194690266, 'model': 'CatBoost,DecisionTreeClassifier,LightGBM,NGBClassifier,XGBoost'}\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fold0</th>\n",
       "      <td>0.913043</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold1</th>\n",
       "      <td>0.565217</td>\n",
       "      <td>XGBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold2</th>\n",
       "      <td>0.608696</td>\n",
       "      <td>LightGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold3</th>\n",
       "      <td>0.909091</td>\n",
       "      <td>CatBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold4</th>\n",
       "      <td>0.954545</td>\n",
       "      <td>NGBClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>0.787611</td>\n",
       "      <td>CatBoost,DecisionTreeClassifier,LightGBM,NGBCl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            acc                                              model\n",
       "fold0  0.913043                             DecisionTreeClassifier\n",
       "fold1  0.565217                                            XGBoost\n",
       "fold2  0.608696                                           LightGBM\n",
       "fold3  0.909091                                           CatBoost\n",
       "fold4  0.954545                                      NGBClassifier\n",
       "all    0.787611  CatBoost,DecisionTreeClassifier,LightGBM,NGBCl..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof = OOF(task='cls', model=['dt', 'xgb', 'lgb', 'cb', 'ngb'], epochs=100)\n",
    "oof.fit(xtrain, ytrain, record_time=True, class_weight=class_weight, weight_train=sample_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x, y = load_iris(return_X_y=True, as_frame=True)\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = {0: 1, 1: 2, 2: 4}\n",
    "sample_weight = ytrain.map(class_weight).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.9066666666666666\n",
      "score: 0.9733333333333334\n",
      "score: 0.9733333333333334\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier(class_weight=None, random_state=42)\n",
    "model.fit(xtrain, ytrain, sample_weight=None)\n",
    "print('score:', model.score(xtest, ytest))\n",
    "\n",
    "# class_weight\n",
    "model = DecisionTreeClassifier(class_weight=class_weight, random_state=42)\n",
    "model.fit(xtrain, ytrain, sample_weight=None)\n",
    "print('score:', model.score(xtest, ytest))\n",
    "ypred1 = model.predict_proba(xtest)\n",
    "\n",
    "# sample_weight\n",
    "model = DecisionTreeClassifier(class_weight=None, random_state=42)\n",
    "model.fit(xtrain, ytrain, sample_weight=sample_weight)\n",
    "print('score:', model.score(xtest, ytest))\n",
    "ypred2 = model.predict_proba(xtest)\n",
    "\n",
    "print((ypred1 == ypred2).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.9866666666666667\n",
      "score: 0.9866666666666667\n",
      "score: 0.9866666666666667\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(class_weight=None, random_state=42)\n",
    "model.fit(xtrain, ytrain, sample_weight=None)\n",
    "print('score:', model.score(xtest, ytest))\n",
    "\n",
    "# class_weight\n",
    "model = RandomForestClassifier(class_weight=class_weight, random_state=42)\n",
    "model.fit(xtrain, ytrain, sample_weight=None)\n",
    "print('score:', model.score(xtest, ytest))\n",
    "ypred1 = model.predict_proba(xtest)\n",
    "\n",
    "# sample_weight\n",
    "model = RandomForestClassifier(class_weight=None, random_state=42)\n",
    "model.fit(xtrain, ytrain, sample_weight=sample_weight)\n",
    "print('score:', model.score(xtest, ytest))\n",
    "ypred2 = model.predict_proba(xtest)\n",
    "\n",
    "print((ypred1 == ypred2).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.96\n",
      "score: 0.9733333333333334\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# X class_weight\n",
    "model = XGBClassifier(scale_pos_weight=None, random_state=42)\n",
    "model.fit(xtrain, ytrain, sample_weight=None)\n",
    "print('score:', model.score(xtest, ytest))\n",
    "ypred1 = model.predict_proba(xtest)\n",
    "\n",
    "# sample_weight\n",
    "model = XGBClassifier(scale_pos_weight=None, random_state=42)\n",
    "model.fit(xtrain, ytrain, sample_weight=sample_weight)\n",
    "print('score:', model.score(xtest, ytest))\n",
    "ypred2 = model.predict_proba(xtest)\n",
    "\n",
    "print((ypred1 == ypred2).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.9733333333333334\n",
      "score: 0.9733333333333334\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# X class_weight\n",
    "model = LGBMClassifier(scale_pos_weight=None, random_state=42, verbosity=-1)\n",
    "model.fit(xtrain, ytrain, sample_weight=None)\n",
    "print('score:', model.score(xtest, ytest))\n",
    "ypred1 = model.predict_proba(xtest)\n",
    "\n",
    "# sample_weight\n",
    "model = LGBMClassifier(scale_pos_weight=None, random_state=42, verbosity=-1)\n",
    "model.fit(xtrain, ytrain, sample_weight=sample_weight)\n",
    "print('score:', model.score(xtest, ytest))\n",
    "ypred2 = model.predict_proba(xtest)\n",
    "\n",
    "print((ypred1 == ypred2).all())  # !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.9733333333333334\n",
      "score: 0.9733333333333334\n",
      "score: 0.9733333333333334\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "model = LGBMClassifier(class_weight=None, random_state=42, verbosity=-1)\n",
    "model.fit(xtrain, ytrain, sample_weight=None)\n",
    "ypred = model.predict_proba(xtest)\n",
    "print('score:', accuracy_score(ytest, ypred.argmax(axis=1)))\n",
    "\n",
    "# class_weight\n",
    "model = LGBMClassifier(class_weight=class_weight, random_state=42, verbosity=-1)\n",
    "model.fit(xtrain, ytrain, sample_weight=None)\n",
    "print('score:', model.score(xtest, ytest))\n",
    "ypred1 = model.predict_proba(xtest)\n",
    "\n",
    "# sample_weight\n",
    "model = LGBMClassifier(class_weight=None, random_state=42, verbosity=-1)\n",
    "model.fit(xtrain, ytrain, sample_weight=sample_weight)\n",
    "print('score:', model.score(xtest, ytest))\n",
    "ypred2 = model.predict_proba(xtest)\n",
    "\n",
    "print((ypred == ypred1).all())\n",
    "print((ypred1 == ypred2).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.9733333333333334\n",
      "score: 0.9866666666666667\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# X class_weight\n",
    "model = CatBoostClassifier(iterations=100, scale_pos_weight=None, random_state=42, logging_level='Silent')\n",
    "model.fit(xtrain, ytrain, sample_weight=None)\n",
    "print('score:', model.score(xtest, ytest))\n",
    "ypred1 = model.predict_proba(xtest)\n",
    "\n",
    "# sample_weight\n",
    "model = CatBoostClassifier(iterations=100, scale_pos_weight=None, random_state=42, logging_level='Silent')\n",
    "model.fit(xtrain, ytrain, sample_weight=sample_weight)\n",
    "print('score:', model.score(xtest, ytest))\n",
    "ypred2 = model.predict_proba(xtest)\n",
    "\n",
    "print((ypred1 == ypred2).all())  # !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.9733333333333334\n",
      "score: 0.9866666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.9866666666666667\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "model = CatBoostClassifier(iterations=100, class_weights=None, random_state=42, logging_level='Silent')\n",
    "model.fit(xtrain, ytrain, sample_weight=None)\n",
    "print('score:', model.score(xtest, ytest))\n",
    "\n",
    "# class_weight\n",
    "model = CatBoostClassifier(iterations=100, class_weights=class_weight, random_state=42, logging_level='Silent')\n",
    "model.fit(xtrain, ytrain, sample_weight=None)\n",
    "print('score:', model.score(xtest, ytest))\n",
    "ypred1 = model.predict_proba(xtest)\n",
    "\n",
    "# sample_weight\n",
    "model = CatBoostClassifier(iterations=100, class_weights=None, random_state=42, logging_level='Silent')\n",
    "model.fit(xtrain, ytrain, sample_weight=sample_weight)\n",
    "print('score:', model.score(xtest, ytest))\n",
    "ypred2 = model.predict_proba(xtest)\n",
    "\n",
    "print((ypred1 == ypred2).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.9733333333333334\n",
      "score: 0.9866666666666667\n",
      "score: 0.9866666666666667\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "model = cb.train(\n",
    "    params={\n",
    "        'loss_function': 'MultiClass',\n",
    "        'class_weights': [1, 1, 1],\n",
    "        'logging_level': 'Silent',\n",
    "    },\n",
    "    dtrain=cb.Pool(xtrain, label=ytrain, weight=None),\n",
    "    iterations=100,\n",
    ")\n",
    "ypred = model.predict(xtest)\n",
    "print('score:', accuracy_score(ytest, ypred.argmax(axis=1)))\n",
    "\n",
    "model = cb.train(\n",
    "    params={\n",
    "        'loss_function': 'MultiClass',\n",
    "        'class_weights': class_weight,\n",
    "        'logging_level': 'Silent',\n",
    "    },\n",
    "    dtrain=cb.Pool(xtrain, label=ytrain, weight=None),\n",
    "    iterations=100,\n",
    ")\n",
    "ypred1 = model.predict(xtest)\n",
    "print('score:', accuracy_score(ytest, ypred1.argmax(axis=1)))\n",
    "\n",
    "model = cb.train(\n",
    "    params={\n",
    "        'loss_function': 'MultiClass',\n",
    "        'class_weights': [1, 1, 1],\n",
    "        'logging_level': 'Silent',\n",
    "    },\n",
    "    dtrain=cb.Pool(xtrain, label=ytrain, weight=sample_weight),\n",
    "    iterations=100,\n",
    ")\n",
    "ypred2 = model.predict(xtest)\n",
    "print('score:', accuracy_score(ytest, (ypred2).argmax(axis=1)))\n",
    "\n",
    "print((ypred1 == ypred2).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Hold-Out Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[CONFLICT] model_str 'dt' is not supported when sklearn_api=False, automatically use sklearn api.\u001b[0m\n",
      "\u001b[33m[CONFLICT] model_str 'rf' is not supported when sklearn_api=False, automatically use sklearn api.\u001b[0m\n",
      "\u001b[33m[CONFLICT] model_str 'ngb' is not supported when sklearn_api=False, automatically use sklearn api.\u001b[0m\n",
      "\u001b[32m[INFO] not provided X_valid and y_valid, auto split train set into train and valid.\u001b[0m\n",
      "\u001b[32m[INFO] hold-out method training begin.\u001b[0m\n",
      "\u001b[32m[INFO] Model DecisionTreeClassifier 1 / 6 training begin.\u001b[0m\n",
      "\u001b[32m[INFO] Model DecisionTreeClassifier 1 / 6 training finish, cost time 0.002 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 1 / 6 model validation scores: {'acc': 0.8666666666666667, 'model': 'DecisionTreeClassifier'}\u001b[0m\n",
      "\u001b[32m[INFO] Model RandomForestClassifier 2 / 6 training begin.\u001b[0m\n",
      "\u001b[32m[INFO] Model RandomForestClassifier 2 / 6 training finish, cost time 0.066 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 2 / 6 model validation scores: {'acc': 0.8666666666666667, 'model': 'RandomForestClassifier'}\u001b[0m\n",
      "\u001b[32m[INFO] Model XGBoost 3 / 6 training begin.\u001b[0m\n",
      "[0]\tvalid-auc:0.93788\n",
      "[99]\tvalid-auc:0.93902\n",
      "\u001b[32m[INFO] Model XGBoost 3 / 6 training finish, cost time 2.639 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 3 / 6 model validation scores: {'acc': 0.8666666666666667, 'model': 'XGBoost'}\u001b[0m\n",
      "\u001b[32m[INFO] Model LightGBM 4 / 6 training begin.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hemingze/miniconda3/envs/mathm/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.759871\tvalid_0's multi_error: 0.133333\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's multi_logloss: 0.759871\tvalid_0's multi_error: 0.133333\n",
      "\u001b[32m[INFO] Model LightGBM 4 / 6 training finish, cost time 0.344 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 4 / 6 model validation scores: {'acc': 0.8666666666666667, 'model': 'LightGBM'}\u001b[0m\n",
      "\u001b[32m[INFO] Model CatBoost 5 / 6 training begin.\u001b[0m\n",
      "0:\tlearn: 1.0951472\ttest: 1.0962260\tbest: 1.0962260 (0)\ttotal: 314us\tremaining: 31.2ms\n",
      "99:\tlearn: 0.7795466\ttest: 0.8347248\tbest: 0.8347248 (99)\ttotal: 15.2ms\tremaining: 0us\n",
      "\n",
      "bestTest = 0.8347247632\n",
      "bestIteration = 99\n",
      "\n",
      "\u001b[32m[INFO] Model CatBoost 5 / 6 training finish, cost time 0.03 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 5 / 6 model validation scores: {'acc': 0.8666666666666667, 'model': 'CatBoost'}\u001b[0m\n",
      "\u001b[32m[INFO] Model NGBClassifier 6 / 6 training begin.\u001b[0m\n",
      "[iter 0] loss=1.0912 val_loss=0.7080 scale=32.0000 norm=109.2548\n",
      "\u001b[32m[INFO] Model NGBClassifier 6 / 6 training finish, cost time 0.203 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 6 / 6 model validation scores: {'acc': 0.8666666666666667, 'model': 'NGBClassifier'}\u001b[0m\n",
      "\u001b[32m[INFO] hold-out method training finish, cost time 3.285 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] total 6 model validation scores:{'acc': 0.8666666666666667, 'model': 'CatBoost,DecisionTreeClassifier,LightGBM,NGBClassifier,RandomForestClassifier,XGBoost'}\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model0</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model1</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model2</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>XGBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model3</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>LightGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model4</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>CatBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model5</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>NGBClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>CatBoost,DecisionTreeClassifier,LightGBM,NGBCl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             acc                                              model\n",
       "model0  0.866667                             DecisionTreeClassifier\n",
       "model1  0.866667                             RandomForestClassifier\n",
       "model2  0.866667                                            XGBoost\n",
       "model3  0.866667                                           LightGBM\n",
       "model4  0.866667                                           CatBoost\n",
       "model5  0.866667                                      NGBClassifier\n",
       "all     0.866667  CatBoost,DecisionTreeClassifier,LightGBM,NGBCl..."
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dm_utils.hom import HOM\n",
    "\n",
    "hom = HOM(task='cls', model=['dt', 'rf', 'xgb', 'lgb', 'cb', 'ngb'], epochs=100)\n",
    "hom.fit(xtrain, ytrain, record_time=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[CONFLICT] model_str 'dt' is not supported when sklearn_api=False, automatically use sklearn api.\u001b[0m\n",
      "\u001b[33m[CONFLICT] model_str 'rf' is not supported when sklearn_api=False, automatically use sklearn api.\u001b[0m\n",
      "\u001b[33m[CONFLICT] model_str 'ngb' is not supported when sklearn_api=False, automatically use sklearn api.\u001b[0m\n",
      "\u001b[32m[INFO] not provided X_valid and y_valid, auto split train set into train and valid.\u001b[0m\n",
      "\u001b[32m[INFO] hold-out method training begin.\u001b[0m\n",
      "\u001b[32m[INFO] Model DecisionTreeClassifier 1 / 6 training begin.\u001b[0m\n",
      "\u001b[32m[INFO] Model DecisionTreeClassifier 1 / 6 training finish, cost time 0.002 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 1 / 6 model validation scores: {'acc': 0.8666666666666667, 'model': 'DecisionTreeClassifier'}\u001b[0m\n",
      "\u001b[32m[INFO] Model RandomForestClassifier 2 / 6 training begin.\u001b[0m\n",
      "\u001b[32m[INFO] Model RandomForestClassifier 2 / 6 training finish, cost time 0.065 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 2 / 6 model validation scores: {'acc': 0.8666666666666667, 'model': 'RandomForestClassifier'}\u001b[0m\n",
      "\u001b[32m[INFO] Model XGBoost 3 / 6 training begin.\u001b[0m\n",
      "[0]\tvalid-auc:0.93562\n",
      "[99]\tvalid-auc:0.93791\n",
      "\u001b[32m[INFO] Model XGBoost 3 / 6 training finish, cost time 2.697 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 3 / 6 model validation scores: {'acc': 0.8666666666666667, 'model': 'XGBoost'}\u001b[0m\n",
      "\u001b[32m[INFO] Model LightGBM 4 / 6 training begin.\u001b[0m\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hemingze/miniconda3/envs/mathm/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's multi_logloss: 0.682746\tvalid_0's multi_error: 0.235294\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's multi_logloss: 0.682746\tvalid_0's multi_error: 0.235294\n",
      "\u001b[32m[INFO] Model LightGBM 4 / 6 training finish, cost time 14.43 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 4 / 6 model validation scores: {'acc': 0.6, 'model': 'LightGBM'}\u001b[0m\n",
      "\u001b[32m[INFO] Model CatBoost 5 / 6 training begin.\u001b[0m\n",
      "0:\tlearn: 1.0950060\ttest: 1.0965407\tbest: 1.0965407 (0)\ttotal: 383us\tremaining: 38ms\n",
      "99:\tlearn: 0.7853090\ttest: 0.8705478\tbest: 0.8705478 (99)\ttotal: 16.9ms\tremaining: 0us\n",
      "\n",
      "bestTest = 0.8705477866\n",
      "bestIteration = 99\n",
      "\n",
      "\u001b[32m[INFO] Model CatBoost 5 / 6 training finish, cost time 0.032 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 5 / 6 model validation scores: {'acc': 0.8666666666666667, 'model': 'CatBoost'}\u001b[0m\n",
      "\u001b[32m[INFO] Model NGBClassifier 6 / 6 training begin.\u001b[0m\n",
      "[iter 0] loss=0.9961 val_loss=0.9808 scale=32.0000 norm=109.2548\n",
      "\u001b[32m[INFO] Model NGBClassifier 6 / 6 training finish, cost time 0.183 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 6 / 6 model validation scores: {'acc': 0.8, 'model': 'NGBClassifier'}\u001b[0m\n",
      "\u001b[32m[INFO] hold-out method training finish, cost time 17.41 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] total 6 model validation scores:{'acc': 0.8666666666666667, 'model': 'CatBoost,DecisionTreeClassifier,LightGBM,NGBClassifier,RandomForestClassifier,XGBoost'}\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model0</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model1</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model2</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>XGBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model3</th>\n",
       "      <td>0.6</td>\n",
       "      <td>LightGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model4</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>CatBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model5</th>\n",
       "      <td>0.8</td>\n",
       "      <td>NGBClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>CatBoost,DecisionTreeClassifier,LightGBM,NGBCl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             acc                                              model\n",
       "model0  0.866667                             DecisionTreeClassifier\n",
       "model1  0.866667                             RandomForestClassifier\n",
       "model2  0.866667                                            XGBoost\n",
       "model3       0.6                                           LightGBM\n",
       "model4  0.866667                                           CatBoost\n",
       "model5       0.8                                      NGBClassifier\n",
       "all     0.866667  CatBoost,DecisionTreeClassifier,LightGBM,NGBCl..."
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hom = HOM(task='cls', model=['dt', 'rf', 'xgb', 'lgb', 'cb', 'ngb'], epochs=100)\n",
    "hom.fit(xtrain, ytrain, record_time=True, class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[CONFLICT] model_str 'dt' is not supported when sklearn_api=False, automatically use sklearn api.\u001b[0m\n",
      "\u001b[33m[CONFLICT] model_str 'rf' is not supported when sklearn_api=False, automatically use sklearn api.\u001b[0m\n",
      "\u001b[33m[CONFLICT] model_str 'ngb' is not supported when sklearn_api=False, automatically use sklearn api.\u001b[0m\n",
      "\u001b[32m[INFO] not provided X_valid and y_valid, auto split train set into train and valid.\u001b[0m\n",
      "\u001b[32m[INFO] hold-out method training begin.\u001b[0m\n",
      "\u001b[32m[INFO] Model DecisionTreeClassifier 1 / 6 training begin.\u001b[0m\n",
      "\u001b[32m[INFO] Model DecisionTreeClassifier 1 / 6 training finish, cost time 0.002 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 1 / 6 model validation scores: {'acc': 0.8666666666666667, 'model': 'DecisionTreeClassifier'}\u001b[0m\n",
      "\u001b[32m[INFO] Model RandomForestClassifier 2 / 6 training begin.\u001b[0m\n",
      "\u001b[32m[INFO] Model RandomForestClassifier 2 / 6 training finish, cost time 0.064 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 2 / 6 model validation scores: {'acc': 0.8666666666666667, 'model': 'RandomForestClassifier'}\u001b[0m\n",
      "\u001b[32m[INFO] Model XGBoost 3 / 6 training begin.\u001b[0m\n",
      "[0]\tvalid-auc:0.93562\n",
      "[99]\tvalid-auc:0.93791\n",
      "\u001b[32m[INFO] Model XGBoost 3 / 6 training finish, cost time 3.346 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 3 / 6 model validation scores: {'acc': 0.8666666666666667, 'model': 'XGBoost'}\u001b[0m\n",
      "\u001b[32m[INFO] Model LightGBM 4 / 6 training begin.\u001b[0m\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hemingze/miniconda3/envs/mathm/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's multi_logloss: 0.682746\tvalid_0's multi_error: 0.235294\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's multi_logloss: 0.682746\tvalid_0's multi_error: 0.235294\n",
      "\u001b[32m[INFO] Model LightGBM 4 / 6 training finish, cost time 1.887 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 4 / 6 model validation scores: {'acc': 0.6, 'model': 'LightGBM'}\u001b[0m\n",
      "\u001b[32m[INFO] Model CatBoost 5 / 6 training begin.\u001b[0m\n",
      "0:\tlearn: 1.0950060\ttest: 1.0965407\tbest: 1.0965407 (0)\ttotal: 258us\tremaining: 25.6ms\n",
      "99:\tlearn: 0.7853090\ttest: 0.8705478\tbest: 0.8705478 (99)\ttotal: 14.3ms\tremaining: 0us\n",
      "\n",
      "bestTest = 0.8705477866\n",
      "bestIteration = 99\n",
      "\n",
      "\u001b[32m[INFO] Model CatBoost 5 / 6 training finish, cost time 0.029 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 5 / 6 model validation scores: {'acc': 0.8666666666666667, 'model': 'CatBoost'}\u001b[0m\n",
      "\u001b[32m[INFO] Model NGBClassifier 6 / 6 training begin.\u001b[0m\n",
      "[iter 0] loss=1.0404 val_loss=0.7699 scale=16.0000 norm=54.6274\n",
      "\u001b[32m[INFO] Model NGBClassifier 6 / 6 training finish, cost time 0.182 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 6 / 6 model validation scores: {'acc': 0.8666666666666667, 'model': 'NGBClassifier'}\u001b[0m\n",
      "\u001b[32m[INFO] hold-out method training finish, cost time 5.512 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] total 6 model validation scores:{'acc': 0.8666666666666667, 'model': 'CatBoost,DecisionTreeClassifier,LightGBM,NGBClassifier,RandomForestClassifier,XGBoost'}\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model0</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model1</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model2</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>XGBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model3</th>\n",
       "      <td>0.6</td>\n",
       "      <td>LightGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model4</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>CatBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model5</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>NGBClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>CatBoost,DecisionTreeClassifier,LightGBM,NGBCl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             acc                                              model\n",
       "model0  0.866667                             DecisionTreeClassifier\n",
       "model1  0.866667                             RandomForestClassifier\n",
       "model2  0.866667                                            XGBoost\n",
       "model3       0.6                                           LightGBM\n",
       "model4  0.866667                                           CatBoost\n",
       "model5  0.866667                                      NGBClassifier\n",
       "all     0.866667  CatBoost,DecisionTreeClassifier,LightGBM,NGBCl..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hom = HOM(task='cls', model=['dt', 'rf', 'xgb', 'lgb', 'cb', 'ngb'], epochs=100)\n",
    "hom.fit(xtrain, ytrain, record_time=True, weight_train=sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[CONFLICT] model_str 'dt' is not supported when sklearn_api=False, automatically use sklearn api.\u001b[0m\n",
      "\u001b[33m[CONFLICT] model_str 'rf' is not supported when sklearn_api=False, automatically use sklearn api.\u001b[0m\n",
      "\u001b[33m[CONFLICT] model_str 'ngb' is not supported when sklearn_api=False, automatically use sklearn api.\u001b[0m\n",
      "\u001b[32m[INFO] not provided X_valid and y_valid, auto split train set into train and valid.\u001b[0m\n",
      "\u001b[32m[INFO] hold-out method training begin.\u001b[0m\n",
      "\u001b[32m[INFO] Model DecisionTreeClassifier 1 / 6 training begin.\u001b[0m\n",
      "\u001b[32m[INFO] Model DecisionTreeClassifier 1 / 6 training finish, cost time 0.002 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 1 / 6 model validation scores: {'acc': 0.6666666666666666, 'model': 'DecisionTreeClassifier'}\u001b[0m\n",
      "\u001b[32m[INFO] Model RandomForestClassifier 2 / 6 training begin.\u001b[0m\n",
      "\u001b[32m[INFO] Model RandomForestClassifier 2 / 6 training finish, cost time 0.065 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 2 / 6 model validation scores: {'acc': 0.8666666666666667, 'model': 'RandomForestClassifier'}\u001b[0m\n",
      "\u001b[32m[INFO] Model XGBoost 3 / 6 training begin.\u001b[0m\n",
      "[0]\tvalid-auc:0.93382\n",
      "[99]\tvalid-auc:0.92096\n",
      "\u001b[32m[INFO] Model XGBoost 3 / 6 training finish, cost time 0.378 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 3 / 6 model validation scores: {'acc': 0.8, 'model': 'XGBoost'}\u001b[0m\n",
      "\u001b[32m[INFO] Model LightGBM 4 / 6 training begin.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hemingze/miniconda3/envs/mathm/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.491243\tvalid_0's multi_error: 0.208333\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's multi_logloss: 0.491243\tvalid_0's multi_error: 0.208333\n",
      "\u001b[32m[INFO] Model LightGBM 4 / 6 training finish, cost time 2.886 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 4 / 6 model validation scores: {'acc': 0.4666666666666667, 'model': 'LightGBM'}\u001b[0m\n",
      "\u001b[32m[INFO] Model CatBoost 5 / 6 training begin.\u001b[0m\n",
      "0:\tlearn: 1.0944103\ttest: 1.0959907\tbest: 1.0959907 (0)\ttotal: 308us\tremaining: 30.5ms\n",
      "99:\tlearn: 0.7746309\ttest: 0.8558903\tbest: 0.8558903 (99)\ttotal: 17.3ms\tremaining: 0us\n",
      "\n",
      "bestTest = 0.8558902591\n",
      "bestIteration = 99\n",
      "\n",
      "\u001b[32m[INFO] Model CatBoost 5 / 6 training finish, cost time 0.068 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 5 / 6 model validation scores: {'acc': 0.6666666666666666, 'model': 'CatBoost'}\u001b[0m\n",
      "\u001b[32m[INFO] Model NGBClassifier 6 / 6 training begin.\u001b[0m\n",
      "[iter 0] loss=0.9515 val_loss=0.7833 scale=16.0000 norm=54.6274\n",
      "\u001b[32m[INFO] Model NGBClassifier 6 / 6 training finish, cost time 0.224 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 6 / 6 model validation scores: {'acc': 0.8666666666666667, 'model': 'NGBClassifier'}\u001b[0m\n",
      "\u001b[32m[INFO] hold-out method training finish, cost time 3.625 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] total 6 model validation scores:{'acc': 0.8, 'model': 'CatBoost,DecisionTreeClassifier,LightGBM,NGBClassifier,RandomForestClassifier,XGBoost'}\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model0</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model1</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model2</th>\n",
       "      <td>0.8</td>\n",
       "      <td>XGBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model3</th>\n",
       "      <td>0.466667</td>\n",
       "      <td>LightGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model4</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>CatBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model5</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>NGBClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>0.8</td>\n",
       "      <td>CatBoost,DecisionTreeClassifier,LightGBM,NGBCl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             acc                                              model\n",
       "model0  0.666667                             DecisionTreeClassifier\n",
       "model1  0.866667                             RandomForestClassifier\n",
       "model2       0.8                                            XGBoost\n",
       "model3  0.466667                                           LightGBM\n",
       "model4  0.666667                                           CatBoost\n",
       "model5  0.866667                                      NGBClassifier\n",
       "all          0.8  CatBoost,DecisionTreeClassifier,LightGBM,NGBCl..."
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hom = HOM(task='cls', model=['dt', 'rf', 'xgb', 'lgb', 'cb', 'ngb'], epochs=100)\n",
    "hom.fit(xtrain, ytrain, record_time=True, class_weight=class_weight, weight_train=sample_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7. Out-of-Fold Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[CONFLICT] model_str 'dt' is not supported when sklearn_api=False, automatically use sklearn api.\u001b[0m\n",
      "\u001b[33m[CONFLICT] model_str 'ngb' is not supported when sklearn_api=False, automatically use sklearn api.\u001b[0m\n",
      "\u001b[32m[INFO] 5-fold training begin.\u001b[0m\n",
      "\u001b[32m[INFO] Model DecisionTreeClassifier, Fold 1 / 5 training begin.\u001b[0m\n",
      "\u001b[32m[INFO] Model DecisionTreeClassifier, Fold 1 / 5 training finish, cost time 0.002 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 1 / 5 fold validation scores: {'acc': 0.8666666666666667, 'model': 'DecisionTreeClassifier'}\u001b[0m\n",
      "\u001b[32m[INFO] Model XGBoost, Fold 2 / 5 training begin.\u001b[0m\n",
      "[0]\tvalid-auc:0.97306\n",
      "[99]\tvalid-auc:1.00000\n",
      "\u001b[32m[INFO] Model XGBoost, Fold 2 / 5 training finish, cost time 3.106 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 2 / 5 fold validation scores: {'acc': 0.9333333333333333, 'model': 'XGBoost'}\u001b[0m\n",
      "\u001b[32m[INFO] Model LightGBM, Fold 3 / 5 training begin.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hemingze/miniconda3/envs/mathm/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.793885\tvalid_0's multi_error: 0.2\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's multi_logloss: 0.793885\tvalid_0's multi_error: 0.2\n",
      "\u001b[32m[INFO] Model LightGBM, Fold 3 / 5 training finish, cost time 0.411 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 3 / 5 fold validation scores: {'acc': 0.8, 'model': 'LightGBM'}\u001b[0m\n",
      "\u001b[32m[INFO] Model CatBoost, Fold 4 / 5 training begin.\u001b[0m\n",
      "0:\tlearn: 1.0948377\ttest: 1.0950641\tbest: 1.0950641 (0)\ttotal: 239us\tremaining: 23.7ms\n",
      "99:\tlearn: 0.7861882\ttest: 0.8183327\tbest: 0.8183327 (99)\ttotal: 14.8ms\tremaining: 0us\n",
      "\n",
      "bestTest = 0.8183326855\n",
      "bestIteration = 99\n",
      "\n",
      "\u001b[32m[INFO] Model CatBoost, Fold 4 / 5 training finish, cost time 0.031 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 4 / 5 fold validation scores: {'acc': 0.9333333333333333, 'model': 'CatBoost'}\u001b[0m\n",
      "\u001b[32m[INFO] Model NGBClassifier, Fold 5 / 5 training begin.\u001b[0m\n",
      "[iter 0] loss=1.0976 val_loss=0.0000 scale=512.0000 norm=1748.0773\n",
      "\u001b[32m[INFO] Model NGBClassifier, Fold 5 / 5 training finish, cost time 0.266 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 5 / 5 fold validation scores: {'acc': 0.8, 'model': 'NGBClassifier'}\u001b[0m\n",
      "\u001b[32m[INFO] 5-fold training finish, cost time 3.817 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] total 5-fold validation scores:{'acc': 0.8666666666666667, 'model': 'CatBoost,DecisionTreeClassifier,LightGBM,NGBClassifier,XGBoost'}\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fold0</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold1</th>\n",
       "      <td>0.933333</td>\n",
       "      <td>XGBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold2</th>\n",
       "      <td>0.8</td>\n",
       "      <td>LightGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold3</th>\n",
       "      <td>0.933333</td>\n",
       "      <td>CatBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold4</th>\n",
       "      <td>0.8</td>\n",
       "      <td>NGBClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>CatBoost,DecisionTreeClassifier,LightGBM,NGBCl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            acc                                              model\n",
       "fold0  0.866667                             DecisionTreeClassifier\n",
       "fold1  0.933333                                            XGBoost\n",
       "fold2       0.8                                           LightGBM\n",
       "fold3  0.933333                                           CatBoost\n",
       "fold4       0.8                                      NGBClassifier\n",
       "all    0.866667  CatBoost,DecisionTreeClassifier,LightGBM,NGBCl..."
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dm_utils import OOF\n",
    "\n",
    "oof = OOF(task='cls', model=['dt', 'xgb', 'lgb', 'cb', 'ngb'], epochs=100)\n",
    "oof.fit(xtrain, ytrain, record_time=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[CONFLICT] model_str 'dt' is not supported when sklearn_api=False, automatically use sklearn api.\u001b[0m\n",
      "\u001b[33m[CONFLICT] model_str 'ngb' is not supported when sklearn_api=False, automatically use sklearn api.\u001b[0m\n",
      "\u001b[32m[INFO] 5-fold training begin.\u001b[0m\n",
      "\u001b[32m[INFO] Model DecisionTreeClassifier, Fold 1 / 5 training begin.\u001b[0m\n",
      "\u001b[32m[INFO] Model DecisionTreeClassifier, Fold 1 / 5 training finish, cost time 0.002 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 1 / 5 fold validation scores: {'acc': 0.8666666666666667, 'model': 'DecisionTreeClassifier'}\u001b[0m\n",
      "\u001b[32m[INFO] Model XGBoost, Fold 2 / 5 training begin.\u001b[0m\n",
      "[0]\tvalid-auc:1.00000\n",
      "[99]\tvalid-auc:1.00000\n",
      "\u001b[32m[INFO] Model XGBoost, Fold 2 / 5 training finish, cost time 3.111 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 2 / 5 fold validation scores: {'acc': 1.0, 'model': 'XGBoost'}\u001b[0m\n",
      "\u001b[32m[INFO] Model LightGBM, Fold 3 / 5 training begin.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hemingze/miniconda3/envs/mathm/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.697607\tvalid_0's multi_error: 0.235294\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's multi_logloss: 0.697607\tvalid_0's multi_error: 0.235294\n",
      "\u001b[32m[INFO] Model LightGBM, Fold 3 / 5 training finish, cost time 2.683 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 3 / 5 fold validation scores: {'acc': 0.5333333333333333, 'model': 'LightGBM'}\u001b[0m\n",
      "\u001b[32m[INFO] Model CatBoost, Fold 4 / 5 training begin.\u001b[0m\n",
      "0:\tlearn: 1.0951741\ttest: 1.0947838\tbest: 1.0947838 (0)\ttotal: 292us\tremaining: 29ms\n",
      "99:\tlearn: 0.8045640\ttest: 0.8304102\tbest: 0.8304102 (99)\ttotal: 17.9ms\tremaining: 0us\n",
      "\n",
      "bestTest = 0.8304101857\n",
      "bestIteration = 99\n",
      "\n",
      "\u001b[32m[INFO] Model CatBoost, Fold 4 / 5 training finish, cost time 0.035 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 4 / 5 fold validation scores: {'acc': 1.0, 'model': 'CatBoost'}\u001b[0m\n",
      "\u001b[32m[INFO] Model NGBClassifier, Fold 5 / 5 training begin.\u001b[0m\n",
      "[iter 0] loss=1.0305 val_loss=0.0000 scale=512.0000 norm=1748.0773\n",
      "\u001b[32m[INFO] Model NGBClassifier, Fold 5 / 5 training finish, cost time 0.298 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 5 / 5 fold validation scores: {'acc': 0.8, 'model': 'NGBClassifier'}\u001b[0m\n",
      "\u001b[32m[INFO] 5-fold training finish, cost time 6.131 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] total 5-fold validation scores:{'acc': 0.84, 'model': 'CatBoost,DecisionTreeClassifier,LightGBM,NGBClassifier,XGBoost'}\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fold0</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>XGBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold2</th>\n",
       "      <td>0.533333</td>\n",
       "      <td>LightGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>CatBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold4</th>\n",
       "      <td>0.8</td>\n",
       "      <td>NGBClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>0.84</td>\n",
       "      <td>CatBoost,DecisionTreeClassifier,LightGBM,NGBCl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            acc                                              model\n",
       "fold0  0.866667                             DecisionTreeClassifier\n",
       "fold1       1.0                                            XGBoost\n",
       "fold2  0.533333                                           LightGBM\n",
       "fold3       1.0                                           CatBoost\n",
       "fold4       0.8                                      NGBClassifier\n",
       "all        0.84  CatBoost,DecisionTreeClassifier,LightGBM,NGBCl..."
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof = OOF(task='cls', model=['dt', 'xgb', 'lgb', 'cb', 'ngb'], epochs=100)\n",
    "oof.fit(xtrain, ytrain, record_time=True, class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[CONFLICT] model_str 'dt' is not supported when sklearn_api=False, automatically use sklearn api.\u001b[0m\n",
      "\u001b[33m[CONFLICT] model_str 'ngb' is not supported when sklearn_api=False, automatically use sklearn api.\u001b[0m\n",
      "\u001b[32m[INFO] 5-fold training begin.\u001b[0m\n",
      "\u001b[32m[INFO] Model DecisionTreeClassifier, Fold 1 / 5 training begin.\u001b[0m\n",
      "\u001b[32m[INFO] Model DecisionTreeClassifier, Fold 1 / 5 training finish, cost time 0.002 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 1 / 5 fold validation scores: {'acc': 0.8666666666666667, 'model': 'DecisionTreeClassifier'}\u001b[0m\n",
      "\u001b[32m[INFO] Model XGBoost, Fold 2 / 5 training begin.\u001b[0m\n",
      "[0]\tvalid-auc:1.00000\n",
      "[99]\tvalid-auc:1.00000\n",
      "\u001b[32m[INFO] Model XGBoost, Fold 2 / 5 training finish, cost time 2.396 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 2 / 5 fold validation scores: {'acc': 1.0, 'model': 'XGBoost'}\u001b[0m\n",
      "\u001b[32m[INFO] Model LightGBM, Fold 3 / 5 training begin.\u001b[0m\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hemingze/miniconda3/envs/mathm/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's multi_logloss: 0.697607\tvalid_0's multi_error: 0.235294\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's multi_logloss: 0.697607\tvalid_0's multi_error: 0.235294\n",
      "\u001b[32m[INFO] Model LightGBM, Fold 3 / 5 training finish, cost time 0.631 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 3 / 5 fold validation scores: {'acc': 0.5333333333333333, 'model': 'LightGBM'}\u001b[0m\n",
      "\u001b[32m[INFO] Model CatBoost, Fold 4 / 5 training begin.\u001b[0m\n",
      "0:\tlearn: 1.0951741\ttest: 1.0947838\tbest: 1.0947838 (0)\ttotal: 302us\tremaining: 29.9ms\n",
      "99:\tlearn: 0.8045640\ttest: 0.8304102\tbest: 0.8304102 (99)\ttotal: 19.1ms\tremaining: 0us\n",
      "\n",
      "bestTest = 0.8304101857\n",
      "bestIteration = 99\n",
      "\n",
      "\u001b[32m[INFO] Model CatBoost, Fold 4 / 5 training finish, cost time 0.039 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 4 / 5 fold validation scores: {'acc': 1.0, 'model': 'CatBoost'}\u001b[0m\n",
      "\u001b[32m[INFO] Model NGBClassifier, Fold 5 / 5 training begin.\u001b[0m\n",
      "[iter 0] loss=1.0743 val_loss=2.0992 scale=512.0000 norm=1748.0773\n",
      "\u001b[32m[INFO] Model NGBClassifier, Fold 5 / 5 training finish, cost time 0.286 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 5 / 5 fold validation scores: {'acc': 0.7333333333333333, 'model': 'NGBClassifier'}\u001b[0m\n",
      "\u001b[32m[INFO] 5-fold training finish, cost time 3.354 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] total 5-fold validation scores:{'acc': 0.8266666666666667, 'model': 'CatBoost,DecisionTreeClassifier,LightGBM,NGBClassifier,XGBoost'}\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fold0</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>XGBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold2</th>\n",
       "      <td>0.533333</td>\n",
       "      <td>LightGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>CatBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold4</th>\n",
       "      <td>0.733333</td>\n",
       "      <td>NGBClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>0.826667</td>\n",
       "      <td>CatBoost,DecisionTreeClassifier,LightGBM,NGBCl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            acc                                              model\n",
       "fold0  0.866667                             DecisionTreeClassifier\n",
       "fold1       1.0                                            XGBoost\n",
       "fold2  0.533333                                           LightGBM\n",
       "fold3       1.0                                           CatBoost\n",
       "fold4  0.733333                                      NGBClassifier\n",
       "all    0.826667  CatBoost,DecisionTreeClassifier,LightGBM,NGBCl..."
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof = OOF(task='cls', model=['dt', 'xgb', 'lgb', 'cb', 'ngb'], epochs=100)\n",
    "oof.fit(xtrain, ytrain, record_time=True, weight_train=sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[CONFLICT] model_str 'dt' is not supported when sklearn_api=False, automatically use sklearn api.\u001b[0m\n",
      "\u001b[33m[CONFLICT] model_str 'ngb' is not supported when sklearn_api=False, automatically use sklearn api.\u001b[0m\n",
      "\u001b[32m[INFO] 5-fold training begin.\u001b[0m\n",
      "\u001b[32m[INFO] Model DecisionTreeClassifier, Fold 1 / 5 training begin.\u001b[0m\n",
      "\u001b[32m[INFO] Model DecisionTreeClassifier, Fold 1 / 5 training finish, cost time 0.002 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 1 / 5 fold validation scores: {'acc': 0.6666666666666666, 'model': 'DecisionTreeClassifier'}\u001b[0m\n",
      "\u001b[32m[INFO] Model XGBoost, Fold 2 / 5 training begin.\u001b[0m\n",
      "[0]\tvalid-auc:0.96854\n",
      "[99]\tvalid-auc:1.00000\n",
      "\u001b[32m[INFO] Model XGBoost, Fold 2 / 5 training finish, cost time 0.365 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 2 / 5 fold validation scores: {'acc': 1.0, 'model': 'XGBoost'}\u001b[0m\n",
      "\u001b[32m[INFO] Model LightGBM, Fold 3 / 5 training begin.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hemingze/miniconda3/envs/mathm/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.418338\tvalid_0's multi_error: 0.0980392\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's multi_logloss: 0.418338\tvalid_0's multi_error: 0.0980392\n",
      "\u001b[32m[INFO] Model LightGBM, Fold 3 / 5 training finish, cost time 2.975 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 3 / 5 fold validation scores: {'acc': 0.5333333333333333, 'model': 'LightGBM'}\u001b[0m\n",
      "\u001b[32m[INFO] Model CatBoost, Fold 4 / 5 training begin.\u001b[0m\n",
      "0:\tlearn: 1.0948456\ttest: 1.0943479\tbest: 1.0943479 (0)\ttotal: 269us\tremaining: 26.7ms\n",
      "99:\tlearn: 0.7993970\ttest: 0.8136178\tbest: 0.8136178 (99)\ttotal: 15.7ms\tremaining: 0us\n",
      "\n",
      "bestTest = 0.8136177785\n",
      "bestIteration = 99\n",
      "\n",
      "\u001b[32m[INFO] Model CatBoost, Fold 4 / 5 training finish, cost time 0.065 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 4 / 5 fold validation scores: {'acc': 0.9333333333333333, 'model': 'CatBoost'}\u001b[0m\n",
      "\u001b[32m[INFO] Model NGBClassifier, Fold 5 / 5 training begin.\u001b[0m\n",
      "[iter 0] loss=1.0053 val_loss=0.7912 scale=16.0000 norm=54.6274\n",
      "\u001b[32m[INFO] Model NGBClassifier, Fold 5 / 5 training finish, cost time 0.229 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] 5 / 5 fold validation scores: {'acc': 0.8, 'model': 'NGBClassifier'}\u001b[0m\n",
      "\u001b[32m[INFO] 5-fold training finish, cost time 3.637 s.\u001b[0m\n",
      "\u001b[36m[SUCEESS] total 5-fold validation scores:{'acc': 0.7866666666666666, 'model': 'CatBoost,DecisionTreeClassifier,LightGBM,NGBClassifier,XGBoost'}\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fold0</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>XGBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold2</th>\n",
       "      <td>0.533333</td>\n",
       "      <td>LightGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold3</th>\n",
       "      <td>0.933333</td>\n",
       "      <td>CatBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold4</th>\n",
       "      <td>0.8</td>\n",
       "      <td>NGBClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>0.786667</td>\n",
       "      <td>CatBoost,DecisionTreeClassifier,LightGBM,NGBCl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            acc                                              model\n",
       "fold0  0.666667                             DecisionTreeClassifier\n",
       "fold1       1.0                                            XGBoost\n",
       "fold2  0.533333                                           LightGBM\n",
       "fold3  0.933333                                           CatBoost\n",
       "fold4       0.8                                      NGBClassifier\n",
       "all    0.786667  CatBoost,DecisionTreeClassifier,LightGBM,NGBCl..."
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof = OOF(task='cls', model=['dt', 'xgb', 'lgb', 'cb', 'ngb'], epochs=100)\n",
    "oof.fit(xtrain, ytrain, record_time=True, class_weight=class_weight, weight_train=sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mathm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
